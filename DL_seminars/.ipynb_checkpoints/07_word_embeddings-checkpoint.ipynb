{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDjJvApPv9fK"
   },
   "source": [
    "# Эмбеддинги (векторные представления) слов\n",
    "\n",
    "### **1. Что такое word embedding?**\n",
    "\n",
    "При работе с текстами нам хотелось бы понимать, как соотносятся между собой слова в языке (например, насколько они похожи или различны — в каком-то релевантном для решаемой задачи смысле). Поэтому мы можем работать не с самими словами в формате строк, а с их числовым — или скорее векторным — представлением.\n",
    "\n",
    "Один из очевидных способов представить слово в виде строки — one-hot encoding. Однако такое представление, с одной стороны, требует огромного ресурса памяти (например, в русском языке более 100000 слов), а, с другой стороны, не даёт содержательной информации о том, как эти слова между собой соотносятся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhUejOLIhER7"
   },
   "source": [
    "### **2. Word2Vec**\n",
    "\n",
    "Краткое напоминание о том, что такое косинусное расстояние между векторами $x$ и $y$, которое можно рассматривать как меру сходства между ними:\n",
    "\n",
    "$similarity(x, y) = \\cos(\\Theta) = \\frac{\\langle x, y \\rangle}{||x|| \\cdot ||y||}$, где $\\Theta$ — угол между векторами.\n",
    "\n",
    "Модель Word2Vec строит такие векторные представления, чтобы векторы похожих слов оказывались близки по косинусному расстоянию. Похожими считаются слова, которые часто встречаются в одном и том же контексте.\n",
    "\n",
    "Есть различные подходы к тому, как обучать векторные представления. Первый из них — CBoW (Continuous Bag of Words), предсказание слова по контексту, и второй — SkipGram, предсказание контекста по слову. Подробнее об архитектуре и модификациях функции потерь можно прочитать [здесь](https://https://arxiv.org/pdf/1301.3781.pdf) и [здесь](https://https://arxiv.org/abs/1310.4546).\n",
    "\n",
    "Рассмотрим word2vec, [реализованный](https://https://radimrehurek.com/gensim/models/word2vec.html) в библиотеке gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRedhwJllh7W"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_8wSbBuzc6g"
   },
   "source": [
    "Посмотрим, какие доступны предобученные модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68_I9Hcrxced"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YpXAqAEzmFw"
   },
   "source": [
    "Среди них есть word2vec-ruscorpora-300 — word2vec, обученный на Национальном корпусе русского языка. Можно выгрузить эту модель и, например, посмотреть на слова, самые близкие к заданным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRIFWbmG0I5q"
   },
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDrCJz2C1mPs"
   },
   "outputs": [],
   "source": [
    "model.most_similar('человек_NOUN', topn=5) #topn — сколько ближайших слов мы хотим получить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7yX9_Md2ZXp"
   },
   "outputs": [],
   "source": [
    "model.most_similar('счастливый_ADJ', topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0w5oEcA185Q"
   },
   "source": [
    "Обратите внимание, что все слова помечены частями речи.\n",
    "\n",
    "Попробуйте найти косинусное расстояние между векторными представлениями слов \"кошка\" и \"собака\", \"дом\" и \"дерево\", \"радостный\" и \"грустный\" (подсказка: вам НЕ нужно руками писать формулу косинусного расстояния)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student.write_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v32KWfdT4oBp"
   },
   "source": [
    "Заметим, что слова, обозначающие объекты разной природы, отличаются гораздо сильнее, чем антонимичные по смыслу слова. Как вы думаете, почему?\n",
    "\n",
    "Проверьте, работают ли с векторными представлениями слов арифметические операции: \"король\" - \"мужчина\" + \"женщина\" = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1RrkA344naB"
   },
   "outputs": [],
   "source": [
    "sorted(model.similar_by_vector(model[\"россия_NOUN\"] - model[\"москва_NOUN\"] + model[\"вашингтон_NOUN\"]), key=lambda x: x[1])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vutdwjwzC9UG"
   },
   "source": [
    "Однако результат зависит от использованного текстового корпуса, и такую предобученную модель вряд ли можно будет использовать, например, для анализа записей из Twitter. Поэтому может возникнуть необходимость обучить модель самостоятельно.\n",
    "\n",
    "В качестве примера рассмотрим \"игрушечный\" датасет, который есть в gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gGIJ4DKC2DX"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "for text in common_texts:\n",
    "  print(text)\n",
    "\n",
    "model_w2v_toy = Word2Vec(common_texts, size=20, min_count=1)\n",
    "model_w2v_toy.wv.most_similar(positive='human', topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwIT6N9wOoTS"
   },
   "source": [
    "Однако для настоящего обучения модели нужен текст очень большого размера. Возьмём, например, Библию (потому что почему бы и нет). Обратите внимание, что перед обучением текст нужно предобработать (выбросить пунктуацию и стоп-слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meDZS9M2rnyt"
   },
   "outputs": [],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXfJy8Gqfrc2"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#эта ячейка будет работать ДОЛГО\n",
    "nltk.download('stopwords')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "regex = re.compile('[^а-яА-Я ё\\-]')\n",
    "stop = stopwords.words('russian')\n",
    "# + ['сказать', 'и', 'твой']\n",
    "\n",
    "data = urllib.request.urlopen('https://raw.githubusercontent.com/somethingneverending/nlp-files/master/Bible.txt')\n",
    "X = []\n",
    "for line in data:\n",
    "  cur = regex.sub('',line.decode('utf-8')).split('\\n')[0].split(' ')\n",
    "  for word in cur:\n",
    "    normal_word = morph.parse(word.lower())[0].normal_form\n",
    "    if word != '' and normal_word not in stop:\n",
    "      X.append(normal_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qmq95Csfr7sJ"
   },
   "outputs": [],
   "source": [
    "X[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ipwEBVdpG88"
   },
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec([X], size=300, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HMnfAG8PHqC"
   },
   "source": [
    "Посмотрите на синонимы тех же слов, которые мы рассматривали для предобученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCBD_ohcgeFK"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXWPgFQ0PwUm"
   },
   "source": [
    "...Кажется, что-то изменилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1xMFuEoRDbt"
   },
   "source": [
    "Визуализируем 55 наиболее встречающихся слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha7IGwL3RbIK"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vocab = [(model_w2v.wv.vocab[x].count, x) for x in model_w2v.wv.vocab]\n",
    "vocab.sort(reverse=True)\n",
    "vocab = vocab[:55]\n",
    "\n",
    "def visualize(vocab):\n",
    "  all_vocab = []\n",
    "  for i in range(len(vocab)):\n",
    "    all_vocab = all_vocab + vocab[i]\n",
    "  emb_tuple = tuple([model_w2v.wv[word] for word in all_vocab])\n",
    "  X_vis = np.vstack(emb_tuple)\n",
    "\n",
    "  model_tsne = TSNE(n_components=2, random_state=0)\n",
    "  np.set_printoptions(suppress=True)\n",
    "\n",
    "  X_tsne = model_tsne.fit_transform(X_vis)\n",
    "  cur = 0\n",
    "\n",
    "  for part in vocab:\n",
    "    word_labels = [word for word in part]\n",
    "    plt.scatter(X_tsne[cur:cur + len(part), 0], X_tsne[cur:cur + len(part), 1])\n",
    "    for i, word in enumerate(word_labels):\n",
    "        plt.annotate(word, (X_tsne[cur + i, 0], X_tsne[cur + i, 1]))\n",
    "    cur += len(part)\n",
    "  plt.show()\n",
    "\n",
    "vocab = [tmp37[1] for tmp37 in vocab]\n",
    "visualize([vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmzmNgHySjGu"
   },
   "source": [
    "Сделайте то же самое, например, для тридцати слов, ближайшим к какому-нибудь слову; для двадцати ближайших и двадцати самых дальних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AA-OIWPKWSS8"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcK6-_olXG_k"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArkJ8BdjXxCi"
   },
   "source": [
    "То же самое можете сделать самостоятельно и с предобученной моделью."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
