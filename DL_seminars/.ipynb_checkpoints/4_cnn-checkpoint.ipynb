{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8qiR1fW4V66"
   },
   "source": [
    "# Семинар 4: ` Keras` и сверточные нейронные сети\n",
    "\n",
    "На семинаре мы при помощи библиотеки `keras` обучим глубокую сверточную нейронную сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TlKzA9s7NxX"
   },
   "source": [
    "Библиотека `keras` — надстройка над фреймворками [Tensorflow](https://www.tensorflow.org/), [Theano](http://deeplearning.net/software/theano/), [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/). Можно использовать в качестве бэкенда любой из этих фреймворков, и код будет исполняться независмо от выбранного бэкенда. По умолчанию `keras` работает с `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "eeuAxwCYEstl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.backend.backend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "WzkOOpbgFSM_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sharif/venvs/machine-learning/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Tensor(\"MatMul:0\", shape=(4, 4), dtype=float32) \n",
      "\n",
      "c: \n",
      " [[0.3700952  0.5945421  0.6218351  0.99630487]\n",
      " [1.440788   3.9316368  1.5436872  3.5115762 ]\n",
      " [0.76885825 1.9206132  1.4031883  2.2035065 ]\n",
      " [2.2189038  5.0032835  2.2884095  5.393161  ]]\n",
      "d: \n",
      " [1.7405457 3.41993   1.8761808 3.7711515]\n",
      "b: \n",
      " [[0.35088933 0.04339218 0.7248771  0.5289496  0.09243751 0.3700952\n",
      "  0.5945421  0.6218351  0.99630487]\n",
      " [0.3477825  0.9994142  0.7173083  0.8437226  0.5117023  1.440788\n",
      "  3.9316368  1.5436872  3.5115762 ]\n",
      " [0.5496589  0.71591187 0.24752462 0.0860393  0.27704608 0.76885825\n",
      "  1.9206132  1.4031883  2.2035065 ]\n",
      " [0.642939   0.9482262  0.4393835  0.7907487  0.94985425 2.2189038\n",
      "  5.0032835  2.2884095  5.393161  ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# соответствует tf.placeholder() или th.tensor.matrix(), th.tensor.tensor3(), и т. д. в зависимости от бэкенда\n",
    "inputs = K.placeholder(shape=(2, 4, 5))\n",
    "inputs = K.placeholder(shape=(None, 4, 5))\n",
    "inputs = K.placeholder(ndim=3)\n",
    "\n",
    "# делать операции над тензорами можно как мы привыкли с tensorflow\n",
    "b = K.random_uniform_variable(shape=(4, 5), low=0, high=1)\n",
    "c = K.random_normal_variable(shape=(4, 5), mean=0, scale=1)\n",
    "\n",
    "a = b + c * K.abs(b)\n",
    "c = K.dot(a, K.transpose(b))\n",
    "d = K.sum(b, axis=1)\n",
    "b = K.concatenate([b, c], axis=-1)\n",
    "\n",
    "# в результате операций над тензорами получили неинициализированный тензор, как и для tensorflow\n",
    "print(c, \"\\n\")\n",
    "\n",
    "# а теперь посчитаем, чему будут равны тензоры для конкретных входных данных\n",
    "print(\"c: \\n\", K.eval(c))\n",
    "print(\"d: \\n\", K.eval(d))\n",
    "print(\"b: \\n\", K.eval(b))\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t651plf_SDfB"
   },
   "source": [
    "# Keras Sequential API\n",
    "Основная структура данных в `Keras` —  это  `model`, способ организовать слои в нейронной сети. Простейший способ создать модель — использовать `Sequential()`. Такая модель представляет из себя стек слоев, идущих последовательно друг за другом, именно такой моделью мы и будем сегодня пользоваться для обучения сверточной нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8Be6HwRHbN9b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# для первого слоя  в последовательности нужно указать размерности входных данных\n",
    "# для других слоев делать это уже не нужно\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# компилируем модель, определяем функцию потерь, метрику качества, метод оптимизации\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "# обучаем модель, указываем количество шагов градиентного спуска, размер батча\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dIPr0B3dVxs"
   },
   "source": [
    "# Keras Callbacks\n",
    "Callbacks — это набор функций, которые можно передать в процедуру обучения модели. Можно использовать колбэки, чтобы следить за метриками во время обучения, что может быть полезно для дебага, сохранять модель и так далее. Список колбэков передается в `.fit()`, релевантные колбэки вызываются на каждой итерации обучения.\n",
    "\n",
    "В `keras` реализовано [много полезных колбэков](https://keras.io/callbacks/), так же есть возможность писать кастомные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "t8Bsuv4ZhuDm"
   },
   "outputs": [],
   "source": [
    "# определим кастомный колбэк, который выводит функции потерь для каждого батча на каждой итерации\n",
    "class LossHistory(keras.callbacks.Callback):        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        # в self.losses будем хранить искомый список функций потерь для батчей, надо его инициализировать\n",
    "        # еще можно вывести номер итерации\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # выполнили один шаг градиентного спуска, самое время вывести вычисленные функции потерь\n",
    "        ### YOUR CODE HERE        \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # закончили обрабатывать батч, надо сохранить лосс, он лежит в logs.get('loss')\n",
    "        ### YOUR CODE HERE        \n",
    "\n",
    "\n",
    "history = LossHistory()\n",
    "model.fit(data,\n",
    "          one_hot_labels,\n",
    "          epochs=10,\n",
    "          batch_size=200,\n",
    "          verbose=0,\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor='loss'), # добавим еще EarlyStopping, реализованный в keras\n",
    "                     history]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oq_W8bZckif9"
   },
   "source": [
    "# Сверточная нейронная сеть для CIFAR-10\n",
    "\n",
    "Датасет CIFAR-10 содержит цветные картинки 32x32, каждая из которых принадлежит одному из 10 классов: __airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck__\n",
    "\n",
    "В задании нужно будет:\n",
    "* Определить архитектуру CNN\n",
    "* Обучить модель\n",
    "* Визуализировать обученные фильтры\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-nuNBH9m3ls"
   },
   "source": [
    "**ОБРАТИТЕ ВНИМАНИЕ:** обучение CNN на CPU для этого задания может занимать достаточно много времени, 0.5-1.5 часа, поэтому будем обучать модель на GPU\n",
    "\n",
    "**Runtime -> Change runtime type** и выберите **GPU** в Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_u4Rwpd8fBIv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DZtgoH5VfBI0"
   },
   "outputs": [],
   "source": [
    "# чистим сессию, чтобы не получить out-of-memory\n",
    "def reset_tf_session():\n",
    "    curr_session = tf.get_default_session()\n",
    "    if curr_session is not None:\n",
    "        curr_session.close()\n",
    "    K.clear_session()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    s = tf.InteractiveSession(config=config)\n",
    "    K.set_session(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4X4OxWejnj46"
   },
   "source": [
    "# Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Vs5Mo1mCfBI3"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w71tZ9tEfBI6"
   },
   "outputs": [],
   "source": [
    "print(\"Train samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ka43TeITfBI9"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1SIWT_T6fBJA"
   },
   "outputs": [],
   "source": [
    "# посмотрим на случайные картинки из датасета\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(False)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "        ax.set_title(cifar10_classes[y_train[random_index, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvM_vzk6ntxw"
   },
   "source": [
    "# Нормализация данных\n",
    "Как мы знаем, нейронные сети любят, когда входные данные нормализованы, нормализуем их: $$x_{norm} = \\frac{x}{255} - 0.5$$\n",
    "\n",
    "Так же нужно преобразовать классы в one-hot encoded векторы с помощью __keras.utils.to_categorical__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FSvuvnTyfBJC"
   },
   "outputs": [],
   "source": [
    "# нормализуем входные данные\n",
    "x_train2 = ### YOUR CODE HERE\n",
    "x_test2 = ### YOUR CODE HERE\n",
    "\n",
    "# преобразуем метки классов в one-hot encoded векторы, должно получиться(?, NUM_CLASSES)\n",
    "y_train2 = ### YOUR CODE HERE\n",
    "y_test2 = ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLK8CZPnopbV"
   },
   "source": [
    "# Архитектура CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4__6vztrfBJF"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2zEKlLkpNCH"
   },
   "source": [
    "Сверточные нейронные сети строятся из нескольких типов слоев:\n",
    "- [Conv2D](https://keras.io/layers/convolutional/#conv2d) - свертка:\n",
    "    - **filters**: количество выходных каналов; \n",
    "    - **kernel_size**: ширина и высота окна для свертки, tuple/list из двух целых чисел;\n",
    "    - **padding**: padding=\"same\" добавляет столько нулей, чтобы выходной канал имел такие же размеры, как и вход, при padding='valid' свертка выполняется только там, где ядро свертки полностью пересекается с входным каналом;\n",
    "    - **activation**: \"relu\", \"tanh\", и т.д.\n",
    "    - **input_shape**: размерности входных данных.\n",
    "- [MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d) - выполняет 2D max pooling.\n",
    "- [Flatten](https://keras.io/layers/core/#flatten) - не затрагивает размерность,  соответствующую размеру батча.\n",
    "- [Dense](https://keras.io/layers/core/#dense) - полносвязный слой.\n",
    "- [Activation](https://keras.io/layers/core/#activation) - применяет функцию активации.\n",
    "- [LeakyReLU](https://keras.io/layers/advanced-activations/#leakyrelu) - применяет функцию активации leaky relu.\n",
    "- [Dropout](https://keras.io/layers/core/#dropout) - применяет dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obhlpRpBsL1t"
   },
   "source": [
    "Размерность входных данных для модели __(None, 32, 32, 3)__ , размерность выходных __(None, 10)__ . __None__ соответствует размерности размера батча.\n",
    "\n",
    "Шаблон модели:\n",
    "\n",
    "```python\n",
    "model = Sequential() \n",
    "model.add(Conv2D(..., input_shape=(32, 32, 3)))  # не забываем определить размерность входа для первого слоя\n",
    "\n",
    "...  # сверточные слои, пулинг, дропаут\n",
    "\n",
    "model.add(Dense(NUM_CLASSES))  \n",
    "model.add(Activation(\"softmax\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_16Uh2L-fBJH"
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    model = Sequential()\n",
    "      \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sAny7mzafBJL"
   },
   "outputs": [],
   "source": [
    "s = reset_tf_session()\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ao3gB8izLUDI"
   },
   "source": [
    "# Обучение модели\n",
    "\n",
    "На GPU 10-20 секунд на эпоху.\n",
    "Добавим в обучение уменьшение learning rate с номером эпохи при помощи callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h78soVObfBJQ"
   },
   "outputs": [],
   "source": [
    "INIT_LR = 5e-3  # стартовый learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "s = reset_tf_session()\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "# определите функцию потреть, метод оптимизации и метрику качества\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# формула, по которой будет меняться learning rate в зависимости от эпохи, передадим ее в LearningRateScheduler callback\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "# будем выводить актуальный learning rate в начале каждой итерации (on_epoch_begin)\n",
    "# для этого нужно написать кастомный callback\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    ### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yIklukijfBJS"
   },
   "outputs": [],
   "source": [
    "# обучаем модель\n",
    "model.fit(\n",
    "    x_train2, y_train2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n",
    "               LrHistory(),],\n",
    "    validation_data=(x_test2, y_test2),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    initial_epoch=0    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TK_eOCmpfBJW"
   },
   "outputs": [],
   "source": [
    "# сохраняем веса модели\n",
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bKFb-mgxfBJa"
   },
   "outputs": [],
   "source": [
    "# загружаем веса (можно не вызывать model.fit, если модель была однажды обучена и веса сохранены)\n",
    "model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzGTGsmANKi1"
   },
   "source": [
    "# Применение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2H6PmfL_fBJd"
   },
   "outputs": [],
   "source": [
    "# вычисляем прогноз для тестовых данных\n",
    "y_pred_test = model.predict_proba(x_test2)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "y_pred_test_max_probas = np.max(y_pred_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "REz2iK6LfBJh"
   },
   "outputs": [],
   "source": [
    "# матрица ошибок и точность\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.imshow(confusion_matrix(y_test, y_pred_test_classes))\n",
    "plt.xticks(np.arange(10), cifar10_classes, rotation=45, fontsize=12)\n",
    "plt.yticks(np.arange(10), cifar10_classes, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred_test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-d7wvTVUfBJj"
   },
   "outputs": [],
   "source": [
    "# посмотрим на предсказания\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_test))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(False)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(x_test[random_index, :])\n",
    "        pred_label = cifar10_classes[y_pred_test_classes[random_index]]\n",
    "        pred_proba = y_pred_test_max_probas[random_index]\n",
    "        true_label = cifar10_classes[y_test[random_index, 0]]\n",
    "        ax.set_title(\"pred: {}\\nscore: {:.3}\\ntrue: {}\".format(\n",
    "               pred_label, pred_proba, true_label\n",
    "        ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtpSzXimNrm8"
   },
   "source": [
    "# Визуализация фильтров\n",
    "\n",
    "Мы хотим найти входные изображения, максимизирующие активации для заданного слоя нейронной сети.\n",
    "\n",
    "Задача очень похожа на задачу обучения весов для нейронной сети: при обучении весов нам нужно найти веса, минимизирующие функцию потерь, для данного входного изображения. Решаем эту задачу градиентным спуском, ищем градиент функции потерь по весам.\n",
    "\n",
    "Теперь мы хотим найти входное изображение, максимизирующее среднее значение активации на заданном слое\n",
    "$$Loss(layer) = Mean(layer) = \\frac{1}{|layer|}\\sum_{x \\in layer}x \\rightarrow \\max    $$\n",
    "\n",
    "Задачи максимизации решаются аналогично задачам минимизации с помощью градиентного подъема\n",
    "\n",
    "$$image_{t+1} = Image_t + \\gamma\\frac{\\partial  Loss}{\\partial   Image_t}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pFqITQfCfBJp"
   },
   "outputs": [],
   "source": [
    "s = reset_tf_session()\n",
    "K.set_learning_phase(0)\n",
    "model = make_model()\n",
    "model.load_weights(\"weights.h5\")  # веса сохранены после обучения model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V4oCGZabfBJr"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hyMAwN0MfBJu"
   },
   "outputs": [],
   "source": [
    "def find_maximum_stimuli(layer_name, is_conv, filter_index, model, iterations=20, step=1., verbose=True):\n",
    "    \n",
    "    def image_values_to_rgb(x):\n",
    "        # нормализуем x: центрируем в 0 (np.mean(x_train2)), убедиться, что std=0.25 (np.std(x_train2))\n",
    "        # теперь х такой же как нормализованный вход нейронной сети\n",
    "        x = ### YOUR CODE HERE        \n",
    "\n",
    "        # преобразуем нормализованный вход обратно к RGB: x = (x_norm + 0.5) * 255\n",
    "        x = ### YOUR CODE HERE\n",
    "    \n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        return x\n",
    "\n",
    "    # placeholder для входного изображения\n",
    "    input_img = model.input\n",
    "    img_width, img_height = input_img.shape.as_list()[1:3]\n",
    "    \n",
    "    # находим выходы заданного слоя\n",
    "    layer_output = list(filter(lambda x: x.name == layer_name, model.layers))[0].output\n",
    "    \n",
    "    # определите функцию потерь, обратите внимание, что мы можем считать ее \n",
    "    # как для сверточного слоя, так и для полносвязного\n",
    "\n",
    "    loss = ### YOUR CODE HERE\n",
    "\n",
    "    # посчитайте градиент функции потерь по входному изображению с помощью  K.gradients\n",
    "\n",
    "    grads = ### YOUR CODE HERE\n",
    "\n",
    "    # normalization trick\n",
    "    grads = grads / (K.sqrt(K.sum(K.square(grads))) + 1e-10)\n",
    "\n",
    "    # считаем градиенты\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # инициализируем входное ихображение: серое изображение со случайным шумом\n",
    "    input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * (0.1 if is_conv else 0.001)\n",
    "\n",
    "    # градиентный подъем\n",
    "    for i in range(iterations):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        if verbose:\n",
    "            print('Current loss value:', loss_value)\n",
    "\n",
    "    # преобразуем изображение в RGB формат\n",
    "    img = image_values_to_rgb(input_img_data[0])\n",
    "    \n",
    "    return img, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1KwH686LfBJx"
   },
   "outputs": [],
   "source": [
    "# sample maximum stimuli\n",
    "def plot_filters_stimuli(layer_name, is_conv, model, iterations=20, step=1., verbose=False):\n",
    "    cols = 8\n",
    "    rows = 2\n",
    "    filter_index = 0\n",
    "    max_filter_index = list(filter(lambda x: x.name == layer_name, model.layers))[0].output.shape.as_list()[-1] - 1\n",
    "    fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "    for i in range(cols):\n",
    "        for j in range(rows):\n",
    "            if filter_index <= max_filter_index:\n",
    "                ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "                ax.grid(False)\n",
    "                ax.axis('off')\n",
    "                loss = -1e20\n",
    "                while loss < 0 and filter_index <= max_filter_index:\n",
    "                    stimuli, loss = find_maximum_stimuli(layer_name, is_conv, filter_index, model,\n",
    "                                                         iterations, step, verbose=verbose)\n",
    "                    filter_index += 1\n",
    "                if loss > 0:\n",
    "                    ax.imshow(stimuli)\n",
    "                    ax.set_title(\"Filter #{}\".format(filter_index))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "naj2ZdlNfBJ1"
   },
   "outputs": [],
   "source": [
    "# maximum stimuli for convolutional neurons\n",
    "conv_activation_layers = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, LeakyReLU):\n",
    "        prev_layer = layer._inbound_nodes[0].inbound_layers[0]\n",
    "        if isinstance(prev_layer, Conv2D):\n",
    "            conv_activation_layers.append(layer)\n",
    "\n",
    "for layer in conv_activation_layers:\n",
    "    print(layer.name)\n",
    "    plot_filters_stimuli(layer_name=layer.name, is_conv=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MnHJEEpxfBJ5"
   },
   "outputs": [],
   "source": [
    "# maximum stimuli for last dense layer\n",
    "last_dense_layer = list(filter(lambda x: isinstance(x, Dense), model.layers))[-1]\n",
    "plot_filters_stimuli(layer_name=last_dense_layer.name, is_conv=False, \n",
    "                     iterations=200, step=0.1, model=model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "4_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
