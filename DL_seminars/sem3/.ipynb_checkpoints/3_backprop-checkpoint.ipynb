{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Современные методы машинного обучения, ИАД\n",
    "\n",
    "## Семинар 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск: цепное правило"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из курса матанализа мы умеем дифферинциировать простые функции, например:\n",
    "\n",
    "$\\frac{dx^2}{dx} = 2x$  \n",
    "\n",
    "$\\frac{de^x}{dx} = e^ x$   \n",
    "\n",
    "$\\frac{dln(x)}{dx} = \\frac{1}{x}$\n",
    "\n",
    "Для использования цепного правила в градиентном спуске нам понадобится дифференциировать сложные функции.\n",
    "\n",
    "Сложная функция — это функция от функции.<br>Если u — функция от x, то есть u=u(x),  а f — функция от u:  f=f(u), то функция y=f(u) — сложная.\n",
    "\n",
    "Возьмем сложную функцию:<br>\n",
    "$z_1 = z_1(x_1, x_2)$<br>\n",
    "$z_2 = z_2(x_1, x_2)$<br>\n",
    "$p = p(z_1, z_2)$<br>\n",
    "\n",
    "где $z_1,z_2,p$ дифференциируемы\n",
    "\n",
    "\n",
    "Применим цепное правило:<br>\n",
    "### $\\frac{\\partial p}{\\partial x_1} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_1} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_1}$\n",
    "### $\\frac{\\partial p}{\\partial x_2} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_2} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_2}$\n",
    "\n",
    "<br>\n",
    "пример для $h(x) = f(x)g(x)$: \n",
    "### $\\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial f} \\frac{\\partial f}{\\partial x} + \\frac{\\partial h}{\\partial g} \\frac{\\partial g}{\\partial x} = g \\frac{\\partial f}{\\partial x} + f \\frac{\\partial g}{\\partial x}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим граф вычисления для нашей композиции:\n",
    "<img src=\"pic1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим из него граф производных где каждому ребру будет прописана производная начала по концу:\n",
    "<img src=\"pic2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно догадаться как работает цепное правило\n",
    "<img src=\"pic3.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Добавим еще один скрытый слой:\n",
    "<img src=\"pic4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим цепное правило несколько раз:\n",
    "<img src=\"pic5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим что из себя представляет каждое слагаемое\n",
    "<img src=\"pic6.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic7.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic8.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic9.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм вычисления производной в графе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic10.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простую нейросеть:\n",
    "<img src=\"pic11.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic12.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "<br>\n",
    "$L = 0.5*(y - z)^2$\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial z} = ? $\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial \\alpha} = ? $\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial \\beta} = ? $\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Упражнение 1 **\n",
    "\n",
    "Сделайте три шага градиентного спуска и заполните таблицу при:\n",
    "$ \\alpha_0 = 0.5 $<br> $ \\beta_0 = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| $x_1$ | $x_2$ | y | s | z | L | $\\frac{\\partial L}{ \\partial z}$ | $\\frac{\\partial \\sigma}{ \\partial s}$ | $\\frac{\\partial L}{ \\partial \\alpha}$ | $\\frac{\\partial L}{ \\partial \\beta}$ | $\\alpha$ | $\\beta$ |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "Задание посвящено реализации различных слоёв нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Реализация слоёв графа вычислений\n",
    "\n",
    "В этом задании мы реализуем граф вычислений для задачи распознавания изображений рукописных цифр на примере датасета [MNIST](http://yann.lecun.com/exdb/mnist/) — в частности, эта часть посвящена реализации всех требующихся для построения графа слоёв.\n",
    "\n",
    "Указанная задача является задачей классификации на $K = 10$ классов, поэтому будем строить граф вычислений, выходной слой которого будет содержать 10 нейронов, $k$-ый из которых вычисляет оценку принадлежности объекта $k$-ому классу. В качестве функционала качества в данной задаче будем использовать **кросс-энтропию**:\n",
    "\n",
    "$$Q(a, X) = -\\frac{1}{l}\\sum_{i=1}^l \\sum_{k=1}^K [y_i = k] \\log a_k(x_i),$$\n",
    "где\n",
    "\n",
    "$X = \\{ (x_i, y_i)\\}_{i=1}^l, \\, y_i \\in \\{1, \\dots, K\\},$ — обучающая выборка,\n",
    "\n",
    "$a(x) = (a_k(x))_{k=1}^K \\in \\mathbb{R}^K$ — прогноз графа вычислений для объекта $x$, состоящий из выходов $K$ нейронов выходного слоя (т.е. $a_k(x)$ — оценка принадлежности объекта $x$ классу $k$, построенная при помощи заданного графа вычислений).\n",
    "\n",
    "Нейрнонные сети обучаются с использованием стохастических методов оптимизации, однако для ускорения обучения и большей стабильности за один проход параметры оптимизируются по батчу — набору из нескольких тренировочных примеров, так же batch_size является дополнительной размерностью для входящих в слой тензоров.\n",
    "\n",
    "Для начала определим класс Layer, реализующий тождественный слой, который будет являться базовым классом для всех последующих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here you can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, ...], returns output data [batch, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "\n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "\n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "\n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1**\n",
    "\n",
    "Используя приведенные прототипы, реализуйте слой, применяющий функцию активации ReLU (Rectified Linear Unit) поэлементно к каждому из входов слоя:\n",
    "$$\\text{ReLU}(z) = \\max (0, z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        This layer does not have any parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform ReLU transformation\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, input_units]\n",
    "        \"\"\"\n",
    "        return NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. ReLU input\n",
    "        \"\"\"\n",
    "        return NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "Используя указанные прототипы, реализуйте полносвязный слой, выход которого вычисляется следующим образом (подробнее в соответствующей [лекции](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture11-dl.pdf)):\n",
    "\n",
    "$$f(v; W, b)= Wv + b, $$\n",
    "\n",
    "где\n",
    "* v — выход предыдущего слоя (вектор размера num_inputs);\n",
    "* W — матрица весов [num_inputs, num_outputs];\n",
    "* b — столбец свободных членов (вектор размера num_outputs).\n",
    "\n",
    "При каждом вызове backward() необходимо расчитать градиенты по выходу, используя chain-rule, и сделать один шаг градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = Wx + b\n",
    "\n",
    "        W: matrix of shape [num_inputs, num_outputs]\n",
    "        b: vector of shape [num_outputs]\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "\n",
    "        return NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Computes d f / d x = d f / d dense * d dense / d x,\n",
    "        where d dense/ d x = weights transposed, and performs\n",
    "        one step of gradient descent on W and b.\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        grad_output: [batch, output units]\n",
    "\n",
    "        Returns: grad_input, gradient of output w.r.t input\n",
    "        \"\"\"\n",
    "        return NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3**\n",
    "\n",
    "Как было сказано ранее, в качестве функционала качества в данной задаче мы будем использовать кросс-энтропию. Используя прототипы ниже, реализуйте вычисление данного функционала и его градиента по выходам графа вычислений.\n",
    "\n",
    "Кросс-энтропия предполагает, что модель для каждого объекта выдает вероятности принадлежности к каждому из $K$ классов, т.е. что для одного объекта все $K$ вероятностей неотрицательны и суммируются в 1. В нашем же случае в построении графа участвуют только полносвязный и ReLU слои, а потому выходы графа не являются вероятностями — как правило, в этом случае прогноз $a(x)$ модели нормируется при помощи функции softmax следующим образом:\n",
    "\n",
    "$$\\text{softmax}(a_k(x)) = \\frac{\\exp(a_k(x))}{\\sum_{k=1}^K \\exp(a_k(x))}.$$\n",
    "\n",
    "При реализации указанных функций предполагается, что переданные в качестве параметров оценки принадлежности объектов классам не являются нормированными (их еще называют логитами), но при вычислении указанных величин используйте указанное выше преобразование для приведения этих оценок к корректному виду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute crossentropy from logits and ids of correct answers\n",
    "    logits shape: [batch_size, num_classes]\n",
    "    y_true: [batch_size]\n",
    "    output is a number\n",
    "    \"\"\"\n",
    "    return NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute crossentropy gradient from logits and ids of correct answers\n",
    "    logits shape: [batch_size, num_classes]\n",
    "    y_true: [batch_size]\n",
    "    \"\"\"\n",
    "    return NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Реализация и применение графа вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы научимся объединять слои в единый граф вычислений, а также использовать его для прямого прохода (вычисления прогнозов на объектах) и обратного прохода (обновление обучаемых параметров графа), после чего у нас появится возможность обучить граф. Для простоты реализации будем считать, что в нашем случае граф вычислений задается как список (python list) слоёв из числа реализованных ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен код для скачивания датасета MNIST с официального сайта. Датасет делится на тренировочную и тестовую части. Тренировочная дополнительно разбивается на тренировочную и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def load_mnist(flatten=False):\n",
    "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    \n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько объектов из этого датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4**\n",
    "\n",
    "Используя прототип ниже, реализуйте прямой и обратный проход по графу вычислений и функцию для получения предсказаний метки класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        layers — list of Layer objects\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute activations of all network layers by applying them sequentially.\n",
    "        Return a list of activations for each layer. \n",
    "        Make sure last activation corresponds to network logits.\n",
    "        \"\"\"\n",
    "        \n",
    "        activations = []\n",
    "        input = X\n",
    "\n",
    "        raise NotImplementedError(\"Implement me plz ;(\")\n",
    "\n",
    "        assert len(activations) == len(self.layers)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use network to predict the most likely class for each sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError(\"Implement me plz ;(\")\n",
    "        \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Train your network on a given batch of X and y.\n",
    "        You first need to run forward to get all layer activations.\n",
    "        Then you can run layer.backward going from last to first layer.\n",
    "\n",
    "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the layer activations\n",
    "        layer_activations = self.forward(X)\n",
    "        layer_inputs = [X] + layer_activations  # layer_input[i] is an input for network[i]\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        # Compute the loss and the initial gradient\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "\n",
    "        # propagate gradients through network layers using .backward\n",
    "        # hint: start from last layer and move to earlier layers\n",
    "        raise NotImplementedError(\"Implement me plz ;(\")\n",
    "\n",
    "        return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "hidden_layers_size = 40\n",
    "layers.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, 10))\n",
    "\n",
    "model = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве должна превысить 97%. \n",
    "\n",
    "Ниже определена функции для итерации по батчам, принимающая на вход картинки, метки классов, а также размер батча и флаг, отвечающий за перемешивание примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, seed=1234):\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    indices = np.arange(len(inputs)).astype(np.int32)\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        batch = indices[start_idx:start_idx + batchsize]\n",
    "        \n",
    "        yield inputs[batch], targets[batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены функции для обучения модели и отслеживания значения loss на тренироворочной части и на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        model.backward(x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(model.predict(X_train) == y_train))\n",
    "    val_log.append(np.mean(model.predict(X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Train accuracy:\", train_log[-1])\n",
    "    print(\"Val accuracy:\", val_log[-1])\n",
    "    plt.plot(train_log, label='train accuracy')\n",
    "    plt.plot(val_log, label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы увидим как с помощью фреймворков с автоматическим дифференцированием, создание нейронных сетей занимает намного меньше времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add() # создайте аналогичную сеть как вы делали выше\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
