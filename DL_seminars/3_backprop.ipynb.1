{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Современные методы машинного обучения, ИАД\n",
    "\n",
    "## Семинар 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск: цепное правило"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из курса матанализа мы умеем дифферинциировать простые функции, например:\n",
    "\n",
    "$\\frac{dx^2}{dx} = 2x$  \n",
    "\n",
    "$\\frac{de^x}{dx} = e^ x$   \n",
    "\n",
    "$\\frac{dln(x)}{dx} = \\frac{1}{x}$\n",
    "\n",
    "Для использования цепного правила в градиентном спуске нам понадобится дифференциировать сложные функции.\n",
    "\n",
    "Сложная функция — это функция от функции.<br>Если u — функция от x, то есть u=u(x),  а f — функция от u:  f=f(u), то функция y=f(u) — сложная.\n",
    "\n",
    "Возьмем сложную функцию:<br>\n",
    "$z_1 = z_1(x_1, x_2)$<br>\n",
    "$z_2 = z_2(x_1, x_2)$<br>\n",
    "$p = p(z_1, z_2)$<br>\n",
    "\n",
    "где $z_1,z_2,p$ дифференциируемы\n",
    "\n",
    "\n",
    "Применим цепное правило:<br>\n",
    "### $\\frac{\\partial p}{\\partial x_1} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_1} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_1}$\n",
    "### $\\frac{\\partial p}{\\partial x_2} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_2} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_2}$\n",
    "\n",
    "<br>\n",
    "пример для $h(x) = f(x)g(x)$: \n",
    "### $\\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial f} \\frac{\\partial f}{\\partial x} + \\frac{\\partial h}{\\partial g} \\frac{\\partial g}{\\partial x} = g \\frac{\\partial f}{\\partial x} + f \\frac{\\partial g}{\\partial x}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим граф вычисления для нашей композиции:\n",
    "<img src=\"pic1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим из него граф производных где каждому ребру будет прописана производная начала по концу:\n",
    "<img src=\"pic2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно догадаться как работает цепное правило\n",
    "<img src=\"pic3.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Добавим еще один скрытый слой:\n",
    "<img src=\"pic4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим цепное правило несколько раз:\n",
    "<img src=\"pic5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим что из себя представляет каждое слагаемое\n",
    "<img src=\"pic6.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic7.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic8.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic9.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм вычисления производной в графе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic10.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простую нейросеть:\n",
    "<img src=\"pic11.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"pic12.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "<br>\n",
    "$L = 0.5*(y - z)^2$\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial z} = ? $\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial \\alpha} = ? $\n",
    "<br>\n",
    "$\\frac{\\partial L}{\\partial \\beta} = ? $\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Упражнение 1 **\n",
    "\n",
    "Сделайте три шага градиентного спуска и заполните таблицу при:\n",
    "$ \\alpha_0 = 0.5 $<br> $ \\beta_0 = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| $x_1$ | $x_2$ | y | s | z | L | $\\frac{\\partial L}{ \\partial z}$ | $\\frac{\\partial \\sigma}{ \\partial s}$ | $\\frac{\\partial L}{ \\partial \\alpha}$ | $\\frac{\\partial L}{ \\partial \\beta}$ | $\\alpha$ | $\\beta$ |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
    "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "Задание посвящено реализации различных слоёв нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Реализация слоёв графа вычислений\n",
    "\n",
    "В этом задании мы реализуем граф вычислений для задачи распознавания изображений рукописных цифр на примере датасета [MNIST](http://yann.lecun.com/exdb/mnist/) — в частности, эта часть посвящена реализации всех требующихся для построения графа слоёв.\n",
    "\n",
    "Указанная задача является задачей классификации на $K = 10$ классов, поэтому будем строить граф вычислений, выходной слой которого будет содержать 10 нейронов, $k$-ый из которых вычисляет оценку принадлежности объекта $k$-ому классу. В качестве функционала качества в данной задаче будем использовать **кросс-энтропию**:\n",
    "\n",
    "$$Q(a, X) = -\\frac{1}{l}\\sum_{i=1}^l \\sum_{k=1}^K [y_i = k] \\log a_k(x_i),$$\n",
    "где\n",
    "\n",
    "$X = \\{ (x_i, y_i)\\}_{i=1}^l, \\, y_i \\in \\{1, \\dots, K\\},$ — обучающая выборка,\n",
    "\n",
    "$a(x) = (a_k(x))_{k=1}^K \\in \\mathbb{R}^K$ — прогноз графа вычислений для объекта $x$, состоящий из выходов $K$ нейронов выходного слоя (т.е. $a_k(x)$ — оценка принадлежности объекта $x$ классу $k$, построенная при помощи заданного графа вычислений).\n",
    "\n",
    "Нейрнонные сети обучаются с использованием стохастических методов оптимизации, однако для ускорения обучения и большей стабильности за один проход параметры оптимизируются по батчу — набору из нескольких тренировочных примеров, так же batch_size является дополнительной размерностью для входящих в слой тензоров.\n",
    "\n",
    "Для начала определим класс Layer, реализующий тождественный слой, который будет являться базовым классом для всех последующих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here you can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, ...], returns output data [batch, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "\n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "\n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "\n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1**\n",
    "\n",
    "Используя приведенные прототипы, реализуйте слой, применяющий функцию активации ReLU (Rectified Linear Unit) поэлементно к каждому из входов слоя:\n",
    "$$\\text{ReLU}(z) = \\max (0, z)$$\n",
    "\n",
    "**Решение**.\n",
    "Производная $ReLU(x)$ равна 1 при $x>0$ и нулю при $x\\leq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        This layer does not have any parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform ReLU transformation\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, input_units]\n",
    "        \"\"\"\n",
    "        return np.where(input > 0, input, np.zeros_like(input))\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. ReLU input\n",
    "        \"\"\"\n",
    "        grad = (input > 0) * np.ones_like(input) + (input <= 0) * np.zeros_like(input)\n",
    "        return grad * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "Используя указанные прототипы, реализуйте полносвязный слой, выход которого вычисляется следующим образом (подробнее в соответствующей [лекции](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture11-dl.pdf)):\n",
    "\n",
    "$$z = Wv + b, $$\n",
    "\n",
    "где\n",
    "* v — выход предыдущего слоя (вектор размера num_inputs);\n",
    "* W — матрица весов [num_inputs, num_outputs];\n",
    "* b — столбец свободных членов (вектор размера num_outputs).\n",
    "\n",
    "При каждом вызове backward() необходимо расчитать градиенты по выходу, используя chain-rule, и сделать один шаг градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение**.\n",
    "Пусть размерность входного вектора $v$ равна $d$, а выходного вектора $z$ равна $m$. Для того, чтобы понимать, чему равна производная по $ij$-му элементу матрицы весов, распишем поподробнее, чему равно значение выхода линейной функции:\n",
    "$$\n",
    "    z_i = \\sum_{j=1}^d W_{ij}v_j + b_i\n",
    "$$\n",
    "\n",
    "Ещё раз вспомним формулу для производной сложной функции. Пусть нам дан градиент функции потерь по входам предыдущего слоя (то есть $\\cfrac{\\partial L}{\\partial f(v; W, b)}$. Назовём его $grad\\_output$. Заметим, что от элемента $W_{ij}$ зависит только $i$-я компонента выходного вектора $z$. Тогда производная по $ij$ элементу матрицы есть\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{\\partial W_{ij}} = \\left(\\cfrac{\\partial L}{\\partial z_i}\\right) \\cfrac{\\partial z_i}{\\partial W_{ij}}\n",
    "$$\n",
    "\n",
    "Теперь взглянем ещё раз на формулу подсчёта $z_i$ и поймём, что это линейная функция относительно $W_{ij}$, и производная $\\cfrac{\\partial z_i}{\\partial W_{ij}} = v_j$. \n",
    "\n",
    "Таким образом, производная функции потерь по матрице весов -- это такая матрица $dW$, что \n",
    "$$\n",
    "    dW_{ij} = grad\\_output[i] \\cdot v[j]\n",
    "$$\n",
    "\n",
    "Таким образом, мы вычислили производную по весам для единичного объекта. Когда $v$ и $z$ -- это батч объектов, формула незначительно изменится следующим образом. Будем полагать, что $v$ и $z$ это матрицы размрностей $batch\\_size \\times d$ и $batch\\_size \\times m$. $i$-я координата-столбец матрицы $z$ начинает зависеть от $W_{ij}$ для каждого объекта батча, поэтому для подсчёта производной $i$-й компоненты выхода по весу $W_{ij}$ нам нужно просуммировать зависимость по всем элементам батча, то есть\n",
    "$$\n",
    "    dW_{ij} = \\sum_{k=1}^{batch\\_size} v[k][j] \\cdot grad\\_output[k][i]\n",
    "$$\n",
    "\n",
    "Вспоминая определение умножения матриц, мы можем переписать это в простой матричной форме:\n",
    "$$\n",
    "    dW = v^T \\cdot grad\\_output\n",
    "$$\n",
    "\n",
    "Для производной по $b$ всё значительно проще. Для одного объекта от $b_i$ зависит лишь $z_i$, при этом при взгляде на определение $z_i$, мы получим\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial z_i}{\\partial b_i} = 1\n",
    "$$\n",
    "\n",
    "Для батча объектов все эти единички нужно просуммировать по объектам:\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{b_i} = \\cfrac{\\partial L}{\\partial z} \\cfrac{\\partial z}{b_i} = \\sum_{k=0}^{batch\\_size} grad\\_output[k][i] \\cdot 1\n",
    "$$\n",
    "\n",
    "Далее разберёмся с производной по входам слоя (по $j$-й координате $v$). Теперь от координаты $v_j$ зависит $z_i$ **для каждого $i$**. Для каждой координаты $z_i$ зависимость от $v_j$ опять же линейная, поэтому производная $\\cfrac{\\partial z_i}{\\partial v_j} = W_{ij}$. Поскольку от $v_j$ зависят все компоненты вектора $z$, необходимо по ним просуммировать. Комбинируя всё с chain rule, получаем:\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{\\partial v_j} = \\sum_{i=1}^{m} \\cfrac{\\partial L}{\\partial z_i}\\cfrac{\\partial z_i}{\\partial v_j} = \\sum_{i=1}^{m} \\cfrac{\\partial L}{\\partial z_i}  W_{ij} = \\sum_{i=1}^{m} grad\\_output[i] \\cdot W_{ij}\n",
    "$$\n",
    "\n",
    "Что на языке умножения матриц можно переписать как\n",
    "\n",
    "$$\n",
    "     \\cfrac{\\partial L}{\\partial v} = grad\\_output^T \\cdot W\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в реализации ниже мы для удобства принимаем $W$ размера $d \\times m$, поэтому в формулах применены транспонирования в соответствии с размерностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = Wx + b\n",
    "\n",
    "        W: matrix of shape [num_inputs, num_outputs]\n",
    "        b: vector of shape [num_outputs]\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        # REMEMBER: input_units = d, output_units = m\n",
    "\n",
    "        self.W = np.random.randn(input_units, output_units)\n",
    "        self.b = np.random.randn(1, output_units)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    " \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        # Wv + b\n",
    "        # v: shape [batch_size, d]\n",
    "        # W: shape [d, m]\n",
    "        # b: shape [m, ]\n",
    "        return input @ self.W + self.b\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Computes d f / d x = d f / d dense * d dense / d x,\n",
    "        where d dense/ d x = weights transposed, and performs\n",
    "        one step of gradient descent on W and b.\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        grad_output: [batch, output units]\n",
    "\n",
    "        Returns: grad_input, gradient of output w.r.t input\n",
    "        \"\"\"\n",
    "        # input: shape [batch_size, d]\n",
    "        # W.T: shape [m, d]\n",
    "        # grad_inputs: shape [batch_size, d]\n",
    "        grad_inputs = grad_output @ self.W.T\n",
    "        \n",
    "        # grad_output: shape [batch_size, m]\n",
    "        # input.T: shape [d, batch_size]\n",
    "        # grad_W: shape [d, m]\n",
    "        grad_W = input.T @ grad_output\n",
    "\n",
    "        # grad_b: shape [m, ]\n",
    "        grad_b =  np.sum(grad_output, axis=0) * 1\n",
    "\n",
    "        # Gradient descent step\n",
    "        self.W -= self.learning_rate * grad_W\n",
    "        self.b -= self.learning_rate * grad_b\n",
    "        return grad_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3**\n",
    "\n",
    "Как было сказано ранее, в качестве функционала качества в данной задаче мы будем использовать кросс-энтропию. Используя прототипы ниже, реализуйте вычисление данного функционала и его градиента по выходам графа вычислений.\n",
    "\n",
    "Кросс-энтропия предполагает, что модель для каждого объекта выдает вероятности принадлежности к каждому из $K$ классов, т.е. что для одного объекта все $K$ вероятностей неотрицательны и суммируются в 1. В нашем же случае в построении графа участвуют только полносвязный и ReLU слои, а потому выходы графа не являются вероятностями — как правило, в этом случае прогноз $z=a(x)$ модели нормируется при помощи функции softmax следующим образом:\n",
    "\n",
    "$$\\text{softmax}(z_k) = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}.$$\n",
    "\n",
    "При реализации указанных функций предполагается, что переданные в качестве параметров оценки принадлежности объектов классам не являются нормированными (их еще называют логитами), но при вычислении указанных величин используйте указанное выше преобразование для приведения этих оценок к корректному виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение**. Напомним формулу для кросс-энтропии для одного объекта\n",
    "$$\n",
    "    L(z, y) = -\\sum_{k=1}^{K} \\log (\\mathrm{softmax}(z))[y = k]\n",
    "$$\n",
    "Где $[y=k]$ это индикатор (равен 1, если это так и нулю иначе) того, что объект принадлежит к классу $k$. Возьмём производную софтмакса по $k$-й компоненте. Сначала перепишем немного лосс:\n",
    "$$\n",
    "    L(z, y) = -\\sum_{k=1}^{K}\\sum_{k=1}^{K} [y = k]\\left(z_k - \\log\\left(\\sum_{l=1}^K \\exp(z_l)\\right)\\right)\n",
    "$$\n",
    "\n",
    "В сумме по всевозможным $k$ только одно слагаемое не равно нулю -- того класса $k$, к которому принадлежит объект. Пользуемся формулой производной сложной функции. Напомним, что производная логарифма это \n",
    "$$\n",
    "    \\cfrac{\\partial \\log(x)}{\\partial x} = \\cfrac{1}{x}\n",
    "$$\n",
    "А производная экспоненты это\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial \\exp(x)}{\\partial x} = \\exp(x)\n",
    "$$\n",
    "Записываем собственно саму производную функции потерь по логитам $z$:\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{\\partial z_j} = -\\sum_{k=1}^{k}[y=k]\\left(1 - \\cfrac{\\partial \\log\\left(\\sum_{l=1}^K \\exp(z_l)\\right)}{\\partial \\left(\\sum_{l=1}^K \\exp(z_l)\\right)} \\cdot \\cfrac{\\partial \\left(\\sum_{l=1}^K \\exp(z_l)\\right)}{\\partial z_j}\\right)\n",
    "$$\n",
    "\n",
    "В сумме по $l$ только одно слагаемое зависит от конкретного логита ($\\exp(z_j)$). Поэтому получаем\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{\\partial z_j} = -[y=k]\\left(1  - \\cfrac{1}{\\sum_{l=1}^K \\exp(z_l)} \\cdot \\exp(z_j)\\right)\n",
    "$$\n",
    "\n",
    "То есть в векторном виде это просто $-1 + \\mathrm{softmax}(z)$ для того $z_j$, что $j$ -- это метка класса у данного объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    # Indicator\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    # softmax\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "\n",
    "    # indicator * (1 - softmax)\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Реализация и применение графа вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы научимся объединять слои в единый граф вычислений, а также использовать его для прямого прохода (вычисления прогнозов на объектах) и обратного прохода (обновление обучаемых параметров графа), после чего у нас появится возможность обучить граф. Для простоты реализации будем считать, что в нашем случае граф вычислений задается как список (python list) слоёв из числа реализованных ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен код для скачивания датасета MNIST с официального сайта. Датасет делится на тренировочную и тестовую части. Тренировочная дополнительно разбивается на тренировочную и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def load_mnist(flatten=False):\n",
    "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    \n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько объектов из этого датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4**\n",
    "\n",
    "Используя прототип ниже, реализуйте прямой и обратный проход по графу вычислений и функцию для получения предсказаний метки класса.\n",
    "\n",
    "**Решение**. Всё просто: сначала прямой проход по сети forward с вычисления input'ов для каждого слоя, а затем -- обратный (backward) для вычисления производных по весам каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        layers — list of Layer objects\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute activations of all network layers by applying them sequentially.\n",
    "        Return a list of activations for each layer. \n",
    "        Make sure last activation corresponds to network logits.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        activations = []\n",
    "        input = X\n",
    "        n_layers = len(self.layers)\n",
    "        \n",
    "        # calculating the inputs of all layers\n",
    "        for i in range(n_layers):\n",
    "            X = self.layers[i].forward(X)\n",
    "            activations.append(X)\n",
    "\n",
    "        assert len(activations) == len(self.layers)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use network to predict the most likely class for each sample.\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)[-1]\n",
    "        return logits.argmax(axis=-1)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Train your network on a given batch of X and y.\n",
    "        You first need to run forward to get all layer activations.\n",
    "        Then you can run layer.backward going from last to first layer.\n",
    "\n",
    "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the layer activations\n",
    "        layer_activations = self.forward(X)\n",
    "        layer_inputs = [X] + layer_activations  # layer_input[i] is an input for network[i]\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        # Compute the loss and the initial gradient\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "        out_der = loss_grad\n",
    "        n_layers = len(self.layers)\n",
    "\n",
    "        for i in np.arange(n_layers)[::-1]:\n",
    "            l = self.layers[i]\n",
    "            \n",
    "            # calculating gradients for each layer from end to begin\n",
    "            # given the gradient from the previous layer\n",
    "            der_so_far = l.backward(layer_inputs[i], out_der)\n",
    "            out_der = der_so_far\n",
    "\n",
    "        return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "hidden_layers_size = 40\n",
    "layers.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, 10))\n",
    "\n",
    "model = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все готово для запуска обучения. Ниже определена функции для итерации по батчам, принимающая на вход картинки, метки классов, а также размер батча и флаг, отвечающий за перемешивание примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, seed=1234):\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    indices = np.arange(len(inputs)).astype(np.int32)\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        batch = indices[start_idx:start_idx + batchsize]\n",
    "        \n",
    "        yield inputs[batch], targets[batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены функции для обучения модели и отслеживания значения loss на тренироворочной части и на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "Train accuracy: 0.90822\n",
      "Val accuracy: 0.9096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for epoch in range(15):\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        model.backward(x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(model.predict(X_train) == y_train))\n",
    "    val_log.append(np.mean(model.predict(X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Train accuracy:\", train_log[-1])\n",
    "    print(\"Val accuracy:\", val_log[-1])\n",
    "    plt.plot(train_log, label='train accuracy')\n",
    "    plt.plot(val_log, label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поажлуй с керасом мы разберёмся в следующий раз. Пока что просто спойлер. =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add() # создайте аналогичную сеть как вы делали выше\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
